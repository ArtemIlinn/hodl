{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Парсинг Сбера"
      ],
      "metadata": {
        "id": "OOcJTWa2rsMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_url = \"https://smart-lab.ru/forum/SBER/page\"\n",
        "page_number = 10944\n",
        "\n",
        "id = 0\n",
        "\n",
        "sber_df = pd.DataFrame(columns=[\"id\", \"username\", \"post\", \"ticker\"])\n",
        "\n",
        "for i in range(15):\n",
        "  url = base_url + str(page_number)\n",
        "  page_number -= 1\n",
        "  # Send a GET request to the URL\n",
        "  response = requests.get(url)\n",
        "\n",
        "\n",
        "  # Check if the request was successful\n",
        "  if response.status_code == 200:\n",
        "      # Parse the HTML content\n",
        "      soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "      comment_elements = soup.find_all('li', attrs={\"class\": \"cm_wrap\", \"data-type\":\"comment\"})\n",
        "\n",
        "\n",
        "      for comment in comment_elements:\n",
        "          comment = str(comment)\n",
        "\n",
        "          comment_soup = BeautifulSoup(comment, \"html.parser\")\n",
        "          username = comment_soup.find('a', class_='a_name').get_text(strip=True)\n",
        "          post = comment_soup.find('div', class_='text').get_text(strip=True)\n",
        "\n",
        "          # # Print the extracted data\n",
        "          # print(\"Username:\", username)\n",
        "          # print(\"Post:\", post)\n",
        "          # print()\n",
        "          # # print(comment)\n",
        "          # # print()\n",
        "          new_row = pd.DataFrame({\n",
        "              \"id\": id,\n",
        "              \"username\": username,\n",
        "              \"post\": post,\n",
        "              \"ticker\": [\"SBER\"]\n",
        "          })\n",
        "          id+=1\n",
        "\n",
        "          sber_df = pd.concat([sber_df, new_row], ignore_index=True)\n",
        "\n",
        "  else:\n",
        "      print(\"Failed to retrieve the webpage\")\n",
        "\n",
        "print(sber_df)\n",
        "\n",
        "sber_df.to_csv('/content/drive/MyDrive/HODL Project/sber_df.csv', index=False)"
      ],
      "metadata": {
        "id": "6xAQ_pafSrOo",
        "outputId": "f9242b38-06da-42e3-ae0d-ad0f012e319f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "      id        username                                               post  \\\n",
            "0      0  Путешественник  Не пора ли хорошенько упасть?1-Смотрим гепы. 1...   \n",
            "1      1   Игорь Родичев  Промежуточный результат волновал бы в одном пр...   \n",
            "2      2   Игорь Родичев  Игорь Родичев, Какую цену прогнозируете на кон...   \n",
            "3      3   Игорь Родичев  Игорь Родичев, Какую цену прогнозируете на кон...   \n",
            "4      4  Алексей Мамаев          самое веселое что никто не вкурил тему)))   \n",
            "..   ...             ...                                                ...   \n",
            "370  370            Chef  Chef, атлична, буш маим пиривотчиком… назначаю...   \n",
            "371  371      Константин              бабки с фонды опять на валюту вынули)   \n",
            "372  372      Константин  Константин, тут или иранский сценарий или сбер...   \n",
            "373  373      Константин  Константин, голосовать надо было раньшев 2000✔...   \n",
            "374  374    Anton Palyhc  Anton Palyhc, я чет в голос с нашего рынка с т...   \n",
            "\n",
            "    ticker  \n",
            "0     SBER  \n",
            "1     SBER  \n",
            "2     SBER  \n",
            "3     SBER  \n",
            "4     SBER  \n",
            "..     ...  \n",
            "370   SBER  \n",
            "371   SBER  \n",
            "372   SBER  \n",
            "373   SBER  \n",
            "374   SBER  \n",
            "\n",
            "[375 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Парсинг Газпром"
      ],
      "metadata": {
        "id": "QW-aDbz5vfLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_url = \"https://smart-lab.ru/forum/GAZP/page\"\n",
        "page_number = 3048\n",
        "\n",
        "id = 0\n",
        "gazprom_df = pd.DataFrame(columns=[\"id\", \"username\", \"post\", \"ticker\"])\n",
        "\n",
        "for i in range(15):\n",
        "  url = base_url + str(page_number)\n",
        "  page_number -= 1\n",
        "  # Send a GET request to the URL\n",
        "  response = requests.get(url)\n",
        "\n",
        "\n",
        "  # Check if the request was successful\n",
        "  if response.status_code == 200:\n",
        "      # Parse the HTML content\n",
        "      soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "      comment_elements = soup.find_all('li', attrs={\"class\": \"cm_wrap\", \"data-type\":\"comment\"})\n",
        "\n",
        "\n",
        "      for comment in comment_elements:\n",
        "          comment = str(comment)\n",
        "\n",
        "          comment_soup = BeautifulSoup(comment, \"html.parser\")\n",
        "          username = comment_soup.find('a', class_='a_name').get_text(strip=True)\n",
        "          post = comment_soup.find('div', class_='text').get_text(strip=True)\n",
        "\n",
        "          # # Print the extracted data\n",
        "          # print(\"Username:\", username)\n",
        "          # print(\"Post:\", post)\n",
        "          # print()\n",
        "          # # print(comment)\n",
        "          # # print()\n",
        "          new_row = pd.DataFrame({\n",
        "              \"id\": id,\n",
        "              \"username\": username,\n",
        "              \"post\": post,\n",
        "              \"ticker\": [\"GAZP\"]\n",
        "          })\n",
        "          id+=1\n",
        "\n",
        "          gazprom_df = pd.concat([gazprom_df, new_row], ignore_index=True)\n",
        "\n",
        "  else:\n",
        "      print(\"Failed to retrieve the webpage\")\n",
        "\n",
        "print(gazprom_df)\n",
        "\n",
        "gazprom_df.to_csv('/content/drive/MyDrive/HODL Project/gazprom_df.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KuLYgwXvq4e",
        "outputId": "b44722d7-b131-4072-b337-7537b404c18c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "      id          username                                               post  \\\n",
            "0      0     Сергей Аноним  1. Компания ужасно (подчёркиваю, ужасно) недоо...   \n",
            "1      1        Константин  Константин, тарьте, кто вам не даёт то? Для че...   \n",
            "2      2        Константин  Константин, тарьте, кто вам не даёт то? Для че...   \n",
            "3      3            Bablos  1. Компания ужасно (подчёркиваю, ужасно) недоо...   \n",
            "4      4         UshiSlona  на dohod.ru ожидают, что ГП выдаст полугодовые...   \n",
            "..   ...               ...                                                ...   \n",
            "370  370  Александр Власов  Акция стоит 1.8 доллара. Ранее и по 10 брали. ...   \n",
            "371  371  Андрей Сергеевич  Андрей Сергеевич, а мне кажется что у меня 1 с...   \n",
            "372  372  Александр Власов  Андрей Сергеевич, МНЕ КАЖКТСЯ ЧТО Я ТУ ОДИН В ...   \n",
            "373  373  Александр Власов  Александр Власов, Ну хорошо, если ты сегодня п...   \n",
            "374  374  Андрей Сергеевич  Александр Власов, Ну хорошо, если ты сегодня п...   \n",
            "\n",
            "    ticker  \n",
            "0     GAZP  \n",
            "1     GAZP  \n",
            "2     GAZP  \n",
            "3     GAZP  \n",
            "4     GAZP  \n",
            "..     ...  \n",
            "370   GAZP  \n",
            "371   GAZP  \n",
            "372   GAZP  \n",
            "373   GAZP  \n",
            "374   GAZP  \n",
            "\n",
            "[375 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Парсинг Яндекс"
      ],
      "metadata": {
        "id": "J_EacTqpwkq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_url = \"https://smart-lab.ru/forum/YNDX/page\"\n",
        "page_number = 430\n",
        "\n",
        "id = 0\n",
        "yandex_df = pd.DataFrame(columns=[\"id\", \"username\", \"post\", \"ticker\"])\n",
        "\n",
        "for i in range(15):\n",
        "  url = base_url + str(page_number)\n",
        "  page_number -= 1\n",
        "  # Send a GET request to the URL\n",
        "  response = requests.get(url)\n",
        "\n",
        "\n",
        "  # Check if the request was successful\n",
        "  if response.status_code == 200:\n",
        "      # Parse the HTML content\n",
        "      soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "      comment_elements = soup.find_all('li', attrs={\"class\": \"cm_wrap\", \"data-type\":\"comment\"})\n",
        "\n",
        "\n",
        "      for comment in comment_elements:\n",
        "          comment = str(comment)\n",
        "\n",
        "          comment_soup = BeautifulSoup(comment, \"html.parser\")\n",
        "          username = comment_soup.find('a', class_='a_name').get_text(strip=True)\n",
        "          post = comment_soup.find('div', class_='text').get_text(strip=True)\n",
        "\n",
        "          # # Print the extracted data\n",
        "          # print(\"Username:\", username)\n",
        "          # print(\"Post:\", post)\n",
        "          # print()\n",
        "          # # print(comment)\n",
        "          # # print()\n",
        "\n",
        "          new_row = pd.DataFrame({\n",
        "              \"id\": id,\n",
        "              \"username\": username,\n",
        "              \"post\": post,\n",
        "              \"ticker\": [\"YNDX\"]\n",
        "          })\n",
        "          id+=1\n",
        "\n",
        "          yandex_df = pd.concat([yandex_df, new_row], ignore_index=True)\n",
        "\n",
        "  else:\n",
        "      print(\"Failed to retrieve the webpage\")\n",
        "\n",
        "print(yandex_df)\n",
        "\n",
        "yandex_df.to_csv('/content/drive/MyDrive/HODL Project/yandex_df.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0NEHKRiwm_E",
        "outputId": "46efded0-69f2-41db-90c0-432a9a8568ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "      id              username  \\\n",
            "0      0  Territory of Trading   \n",
            "1      1    Александр Сережкин   \n",
            "2      2             Дмитрий М   \n",
            "3      3            Nordstream   \n",
            "4      4                   R B   \n",
            "..   ...                   ...   \n",
            "370  370      Александр Осипов   \n",
            "371  371            Роман Жмак   \n",
            "372  372               Дмитрий   \n",
            "373  373      Станислав Швецов   \n",
            "374  374        Николай Иванов   \n",
            "\n",
            "                                                  post ticker  \n",
            "0    ✅YandexЯрче подтверждается завершение коррекци...   YNDX  \n",
            "1    Гендиректору компании \"Яндекс\" грозит штрафАдм...   YNDX  \n",
            "2                                      Дорога на 2100.   YNDX  \n",
            "3    Яндекс запустил B2B-сервис Яндекс Командировки...   YNDX  \n",
            "4    Путин назвал обнадеживающими экономические пок...   YNDX  \n",
            "..                                                 ...    ...  \n",
            "370  ЯНДЕКС. ВЫХОД ИЗ ТРЕУГОЛЬНИКА ВВЕРХ. РАЗБОРЕще...   YNDX  \n",
            "371  Яндекс раскрыл сумму сделки от продажи Дзена V...   YNDX  \n",
            "372                                                  .   YNDX  \n",
            "373  \"Яндекс\" на \"маркете\" - кто купит контроль в р...   YNDX  \n",
            "374  Николай Иванов,… Причина +165 Млрд. руб в прод...   YNDX  \n",
            "\n",
            "[375 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Парсинг Магнит"
      ],
      "metadata": {
        "id": "qKNyQ2syxD-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "base_url = \"https://smart-lab.ru/forum/MGNT/page\"\n",
        "page_number = 648\n",
        "\n",
        "id = 0\n",
        "magnit_df = pd.DataFrame(columns=[\"id\", \"username\", \"post\", \"ticker\"])\n",
        "\n",
        "for i in range(15):\n",
        "  url = base_url + str(page_number)\n",
        "  page_number -= 1\n",
        "  # Send a GET request to the URL\n",
        "  response = requests.get(url)\n",
        "\n",
        "\n",
        "  # Check if the request was successful\n",
        "  if response.status_code == 200:\n",
        "      # Parse the HTML content\n",
        "      soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "      comment_elements = soup.find_all('li', attrs={\"class\": \"cm_wrap\", \"data-type\":\"comment\"})\n",
        "\n",
        "\n",
        "      for comment in comment_elements:\n",
        "          comment = str(comment)\n",
        "\n",
        "          comment_soup = BeautifulSoup(comment, \"html.parser\")\n",
        "          username = comment_soup.find('a', class_='a_name').get_text(strip=True)\n",
        "          post = comment_soup.find('div', class_='text').get_text(strip=True)\n",
        "\n",
        "          new_row = pd.DataFrame({\n",
        "              \"id\": id,\n",
        "              \"username\": username,\n",
        "              \"post\": post,\n",
        "              \"ticker\": [\"MGNT\"]\n",
        "          })\n",
        "          id+=1\n",
        "\n",
        "          magnit_df = pd.concat([magnit_df, new_row], ignore_index=True)\n",
        "\n",
        "  else:\n",
        "      print(\"Failed to retrieve the webpage\")\n",
        "\n",
        "print(magnit_df)\n",
        "\n",
        "magnit_df.to_csv('/content/drive/MyDrive/HODL Project/magnit_df.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aB3b9X9gxFt6",
        "outputId": "c9ec1413-546a-41da-ad0f-2aab7e464b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "      id           username  \\\n",
            "0      0     Карим Тактуров   \n",
            "1      1           Sergey Z   \n",
            "2      2               znak   \n",
            "3      3  Владимир Литвинов   \n",
            "4      4            РоманП.   \n",
            "..   ...                ...   \n",
            "370  370                Max   \n",
            "371  371  Константин Коблов   \n",
            "372  372      Heinrich Baur   \n",
            "373  373  Константин Коблов   \n",
            "374  374                Max   \n",
            "\n",
            "                                                  post ticker  \n",
            "0    🥦 Продуктовый ритейл: X5, «Магнит», «Лента» и ...   MGNT  \n",
            "1    ✔️ Магнит — сделка закрыта.Сегодня была закрыт...   MGNT  \n",
            "2    ✔️ Магнит — сделка закрыта.Сегодня была закрыт...   MGNT  \n",
            "3    ​​Магнит - размагнитилсяДисклеймер: Статья выш...   MGNT  \n",
            "4    По приколу этого кроме акций и фьючей в Лонг, ...   MGNT  \n",
            "..                                                 ...    ...  \n",
            "370  Max, перед тем как в подвале прятаться — не за...   MGNT  \n",
            "371  Константин Коблов, иди ты в жопу, НАТО проводи...   MGNT  \n",
            "372  Константин Коблов, иди ты в жопу, НАТО проводи...   MGNT  \n",
            "373  Константин Коблов, иди ты в жопу, НАТО проводи...   MGNT  \n",
            "374  Перспективы российской экономики улучшаются на...   MGNT  \n",
            "\n",
            "[375 rows x 4 columns]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}